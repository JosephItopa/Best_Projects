{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffef944-8a2a-4dca-85a3-311a66dd560d",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization refers to the procedure of splitting a sentence into its constituent\n",
    "partsâ€”the words and punctuation that it is made up of. It is different from simply\n",
    "splitting the sentence on whitespaces, and instead actually divides the sentence\n",
    "into constituent words, numbers (if any), and punctuation, which may not always\n",
    "be separated by whitespaces.\n",
    "\n",
    "NLTK provides a method called word_tokenize() , which tokenizes given text into\n",
    "words. It actually separates the text into different words based on punctuation and\n",
    "spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d78c4f-d002-42dd-84dd-bd5fb4c8b0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download words\n",
    "from nltk import word_tokenize, download\n",
    "download(['punkt','averaged_perceptron_tagger','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f38eb6-5889-49a9-8aec-d02a50c7fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize a sentence\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "print(get_tokens(\"I am reading NLP Fundamentals.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c8043-214b-48b5-8bcf-1fe60d4c77f7",
   "metadata": {},
   "source": [
    "#### POS - Part of Speech\n",
    "\n",
    "In NLP, the term PoS refers to parts of speech. PoS tagging refers to the process\n",
    "of tagging words within sentences with their respective PoS.\n",
    "\n",
    "DT = Determiner\n",
    "NN = Noun, common, singular or mass\n",
    "VBZ = Verb, present tense, third-person singular\n",
    "JJ = Adjective\n",
    "\n",
    "PoS tagging finds application in many NLP tasks, including word sense\n",
    "disambiguation, classification, Named Entity Recognition (NER), and coreference\n",
    "resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c31d32-5a21-4fd0-9176-67cd6de0eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word_tokenize and pos_tag\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13171187-d1ab-4589-98ac-2e81609f1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0be21a-7b2a-4478-9fc8-11dfd63a7aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
     ]
    }
   ],
   "source": [
    "# apply the get_token function to tokenize sentence\n",
    "words = get_tokens(\"I am reading NLP Fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910cd72d-4b4c-4a16-b0f5-48ddf4ba3f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag sentence with PoS\n",
    "def get_pos(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "get_pos(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5cc97-aabf-49ff-a6db-1d7625daf440",
   "metadata": {},
   "source": [
    "Here, PRP stands for personal pronoun, VBP stands for verb present, VGB stands\n",
    "for verb gerund, NNP stands for proper noun singular, and NNS stands for\n",
    "noun plural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb487395-0c8d-4197-bf27-f2feb8ffac0a",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "\n",
    "Stop words are the most frequently occurring words in any language and they are just\n",
    "used to support the construction of sentences and do not contribute anything to the\n",
    "semantics of a sentence.\n",
    "\n",
    "Examples of stop words include \"a,\" \"am,\" \"and,\" \"the,\"\n",
    "\"in,\" \"of,\" and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3933d6a6-9cf7-4889-86ae-178760bba183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b3fc91-3733-4d53-809e-81089fbb613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7f6af5-2de7-4da7-b75f-6faaaf8adbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da940016-d767-454a-99bb-c0abdfb2da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am learning Python. It is one of the \"\\\n",
    "            \"most popular programming languages\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fc215f-7250-453e-bdc1-06187548fa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3621a8a9-3d51-416c-b7bb-d69d0baac33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence_words, stop_words):\n",
    "    return ' '.join([word for word in sentence_words if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e297f87-c1dd-43b3-92f7-689fa03030f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages\n"
     ]
    }
   ],
   "source": [
    "print(remove_stop_words(sentence_words, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85e3758-929d-4137-9ac1-c18237d7f38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning Python . popular programming languages\n"
     ]
    }
   ],
   "source": [
    "stop_words.extend(['I', 'It', 'one'])\n",
    "\n",
    "print(remove_stop_words(sentence_words,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21fbac6-018d-4ad0-a10c-8ef1b5722aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'learning Python . popular programming languages Universe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment - tokenize and remove stop word using a python function\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sentence = \"I am to be learning Python. It is one of the \"\\\n",
    "            \"most popular programming languages in the Universe\"\n",
    "\n",
    "def tokenize_and_stop_word(sentence, stop_words):\n",
    "    token = word_tokenize(sentence)\n",
    "    stop_words.extend(['I', 'It', 'one'])\n",
    "    return ' '.join([word for word in token if word not in stop_words])\n",
    "\n",
    "tokenize_and_stop_word(sentence, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ac328-a133-4ff2-8f3c-a9ab43731306",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "text normalization is a process wherein different variations of text get converted into a\n",
    "standard form.\n",
    "\n",
    "For example, words such as \"does\" and \"doing,\" when converted to their base form, become \"do.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794667dd-1f4b-49e4-a67d-ec6284428d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I visited the US from the UK on 22-10-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9453fd9-2607-416b-b678-bb778205e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.replace(\"US\", \"United States\")\\\n",
    "            .replace(\"UK\", \"United Kingdom\")\\\n",
    "            .replace(\"-18\", \"-2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dec0ce4-e6db-48e6-85c5-7fdf0c6a03c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited the United States from the United Kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "normalized_sentence = normalize(sentence)\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dacba5-ad86-4031-b752-a01e5c7d9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States and United Kingdom are two superpowers\n"
     ]
    }
   ],
   "source": [
    "normalized_sentence = normalize('US and UK are two superpowers')\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120727b6-7e51-48aa-9a78-4dcfefbb1dcb",
   "metadata": {},
   "source": [
    "### Spelling correction is executed in two steps:\n",
    "\n",
    "1. Identify the misspelled word, which can be done by a simple dictionary lookup.\n",
    "If there is no match found in the language dictionary, it is considered to\n",
    "be misspelled.\n",
    "\n",
    "2. Replace it or suggest the correctly spelled word. There are a lot of algorithms for\n",
    "this task. One of them is the minimum edit distance algorithm, which chooses\n",
    "the nearest correctly spelled word for a misspelled word.\n",
    "\n",
    "We make use of the autocorrect Python library to correct spellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e394f39a-fb55-4958-a864-1e590dcf790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d31d87-7f44-4085-b1b8-d39f51b81a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = Speller(lang = 'en')\n",
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c1fc4e-60df-480f-9b88-b37bc87c7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = word_tokenize(\"Ntural Luanguage Processin deals with \"\\\n",
    "                         \"the art of extracting insightes from \"\\\n",
    "                         \"Natural Languaes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994e7e50-c970-43b9-915e-a64ea9604d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25307a31-428f-4115-b4a7-5825e4cbdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(tokens):\n",
    "    sentence_corrected = ' '.join([spell(word) \\\n",
    "                                   for word in tokens])\n",
    "    return sentence_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9758a14b-5682-4eec-9eae-c8aef3a763b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing deals with the art of extracting insights from Natural Languages\n"
     ]
    }
   ],
   "source": [
    "print(correct_spelling(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaed8e1-5378-4203-a883-acc5962f98cd",
   "metadata": {},
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeedf31-df26-4769-bce1-f8fb42d6a34f",
   "metadata": {},
   "source": [
    "It is necessary to convert these words into their base forms, as they carry\n",
    "the same meaning in any case. Stemming is the process that helps us to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d149ae75-9207-46d8-b77b-cf9a4b7dd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabfa6a0-da16-490e-8980-91073f45ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stems(word, stemmer):\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f427a6b-bd21-4147-be70-a94f90220752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porterStem = stem.PorterStemmer()\n",
    "get_stems(\"production\", porterStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef628b6-287f-4581-b0f6-5eb5b59b0344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stems(\"coming\", porterStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09bc382e-802c-4b61-9e99-7f649684acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stems(\"firing\", porterStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34cdd126-469e-4b59-954f-ac4ff69d073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stems(\"battling\", porterStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69d5f4a6-3666-4d9f-a99c-02ccf05445ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = stem.SnowballStemmer(\"english\")\n",
    "get_stems(\"battling\",stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93d4ca-fc9f-4e65-9086-51de752dba5f",
   "metadata": {},
   "source": [
    "### LEMMATIZATION\n",
    "\n",
    "Sometimes, the stemming process leads to incorrect results. For example, in the\n",
    "last exercise, the word battling was transformed to \"battl\" , which is not a\n",
    "word. \n",
    "\n",
    "To overcome such problems with stemming, we make use of lemmatization.\n",
    "\n",
    "Lemmatization is the process of converting words to their base grammatical form,\n",
    "as in \"battling\" to \"battle,\" rather than just randomly axing words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad9f7c-0548-4df3-8437-57a8dd0f84e3",
   "metadata": {},
   "source": [
    "NLTK's WordNetLemmatizer provides a method called lemmatize(),\n",
    "\n",
    "which returns the lemma (grammatical base form) of a given word using WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81fffa1-5d3f-4222-adb7-15671cd6a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49674c90-1789-4ced-a7c7-f2c33bbe6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemma(word):\n",
    "    return lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77c5dc92-617e-40d8-a9c6-216196be45e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be2e215c-a2cb-486c-a688-11152a3ca7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86bf8b-03c2-40bc-83b1-a1750ba8c598",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "NER is the process of extracting important entities, such as person names, place\n",
    "names, and organization names, from some given text. These are usually not present\n",
    "in dictionaries. \n",
    "\n",
    "So, we need to treat them differently. The main objective of this\n",
    "process is to identify the named entities (such as proper nouns) and map them to\n",
    "categories, which are already defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af582a7-91d4-4b5b-b555-35ace68eb1e7",
   "metadata": {},
   "source": [
    "from nltk import download\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "download('maxent_ne_chunker')\n",
    "download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3078c5ed-b870-479d-a96e-bc1d57ae2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We are reading a book published by Packt \"\\\n",
    "            \"which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db7e0465-3321-44e9-8085-b369998a503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    i = ne_chunk(pos_tag(word_tokenize(text)), binary = True)\n",
    "    return [a for a in i if len(a) == 1]\n",
    "\n",
    "get_ner(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0b34-f5f9-40fa-b2ea-74350afda4cf",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation\n",
    "\n",
    "This means two or more words with the same spelling may have different meanings\n",
    "in different contexts. \n",
    "\n",
    "This often leads to ambiguity. Word sense disambiguation\n",
    "is the process of mapping a word to the sense that it should carry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca677218-79a7-4206-88c3-46689cdcd207",
   "metadata": {},
   "source": [
    "One of the algorithms to solve word sense disambiguation is the LESK algorithm.\n",
    "\n",
    "It has a huge corpus in the background (generally WordNet is used) that contains\n",
    "\n",
    "definitions of all the possible synonyms of all the possible words in a language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29e4733d-213b-4648-9954-1df320ac92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80cd8cc5-2043-4b49-826c-4dc914d1ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Keep your savings in the bank\"\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f8a280f-9573-433f-9735-4fbb945ee070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('savings_bank.n.02')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_synset(sentence, word):\n",
    "    return lesk(word_tokenize(sentence), word)\n",
    "\n",
    "get_synset(sentence1,'bank') # Here, savings_bank.n.02 refers to a container for keeping money safely at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd25ad09-1c25-4c51-bc47-6272d596cbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('bank.v.07')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synset(sentence2,'bank') # Here, bank.v.07 refers to a slope in the turn of a road"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefebb9-3807-47ba-bced-4b4136d1dcf2",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Sentence boundary detection is the method of detecting where one sentence ends\n",
    "and another begins. \n",
    "\n",
    "If you are thinking that this sounds pretty easy, as a period (.)\n",
    "or a question mark (?) denotes the end of a sentence and the beginning of another\n",
    "sentence, then you are wrong. \n",
    "\n",
    "There can also be instances where the letters of\n",
    "acronyms are separated by full stops, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78290ffd-f7d3-4d08-9c61-39120e12c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97c1372d-f08d-4b07-b21a-a2790769d556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you know who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birmingham.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "get_sentences(\"We are reading a book. Do you know who is \"\\\n",
    "              \"the publisher? It is Packt. Packt is based \"\\\n",
    "              \"out of Birmingham.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0993eb37-2c6d-4721-8e63-106e4ec0f997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Donald John Trump is the current president of the USA.',\n",
       " 'Before joining politics, he was a businessman.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(\"Mr. Donald John Trump is the current \"\\\n",
    "              \"president of the USA. Before joining \"\\\n",
    "              \"politics, he was a businessman.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e286819-0cb4-48cb-9d62-fc63b077ace9",
   "metadata": {},
   "source": [
    "As you can see in the code, the sent_tokenize method is able to differentiate\n",
    "\n",
    "between the period (.) after \"Mr\" and the one used to end the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223ffd7-51fc-43e3-a7ce-23ca1ae1a16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75e908-438b-4bbe-a834-67bb79c4304e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
