{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36af165f-0d40-4dfc-a227-49a5a4b17fce",
   "metadata": {},
   "source": [
    "### The text corpus, file.txt\n",
    "\n",
    "Follow these steps to implement this activity:\n",
    "\n",
    "1. Import the necessary libraries.\n",
    "\n",
    "2. Load the text corpus to a variable.\n",
    "\n",
    "3. Apply the tokenization process to the text corpus and print the first 20 tokens.\n",
    "\n",
    "4. Apply spelling correction on each token and print the initial 20 corrected tokens\n",
    "as well as the corrected text corpus.\n",
    "\n",
    "5. Apply PoS tags to each of the corrected tokens and print them.\n",
    "\n",
    "6. Remove stop words from the corrected token list and print the initial 20 tokens.\n",
    "\n",
    "7. Apply stemming and lemmatization to the corrected token list and then print the\n",
    "initial 20 tokens.\n",
    "\n",
    "8. Detect the sentence boundaries in the given text corpus and print the total\n",
    "number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df9f9f5-4786-4747-93c3-cef46ea57053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Import the necessary libraries\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import stem\n",
    "from nltk import ne_chunk\n",
    "from nltk.wsd import lesk\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9addfc10-8a2b-4af1-a5da-c113303de227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/josephitopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('stopwords')\n",
    "download(['punkt','averaged_perceptron_tagger','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4017b343-58ef-4830-9b5d-79440f1b42ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reader of this course should have a basic knowledge of the Python programming lenguage.\\nHe/she must have knowldge of data types in Python.He should be able to write functions,\\nand also have the ability to import and use libraries and packages in Python. Familiarity\\nwith basic linguistics and probability is assumed although not required to fully\\ncomplete this course.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2)  Load the text corpus to a variable\n",
    "sentence = open(\"file.txt\", 'r').read()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c5e51e6-a06e-46bd-89f2-fa3afdcaaf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'reader',\n",
       " 'of',\n",
       " 'this',\n",
       " 'course',\n",
       " 'should',\n",
       " 'have',\n",
       " 'a',\n",
       " 'basic',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'lenguage',\n",
       " '.',\n",
       " 'He/she',\n",
       " 'must',\n",
       " 'have',\n",
       " 'knowldge']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) apply tokenization process\n",
    "sentence_token = word_tokenize(sentence)\n",
    "sentence_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0430c49-e04b-4fdf-87cb-95d2d5a58c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus.\n",
    "spell = Speller(lang = 'en')\n",
    "sentence_corrected = ' '.join([spell(word) for word in sentence_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8659040-12d4-4414-a0d0-8e5496bb819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5)  Correct sentence and words\n",
    "spell = Speller(lang = 'en')\n",
    "\n",
    "def correct_sentence(words):\n",
    "    corrected_sentence = \"\"\n",
    "    corrected_word_list = []\n",
    "    for wd in words:\n",
    "        if wd not in string.punctuation:\n",
    "            wd_c = spell(wd)\n",
    "            if wd_c != wd:\n",
    "                print(wd+\" has been corrected to: \"+wd_c)\n",
    "                corrected_sentence = corrected_sentence+\" \"+wd_c\n",
    "                corrected_word_list.append(wd_c)\n",
    "            else:\n",
    "                corrected_sentence = corrected_sentence+\" \"+wd\n",
    "                corrected_word_list.append(wd)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    return corrected_sentence, corrected_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65058158-88ac-4adc-8a4c-55891a6e68f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenguage has been corrected to: language\n",
      "knowldge has been corrected to: knowledge\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence, corrected_word_list = correct_sentence(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3399098-e06b-4f70-972b-a6f2bc624348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reader of this course should have a basic knowledge of the Python programming language. He/she must have knowledge of data types in Python.He should be able to write functions, and also have the ability to import and use libraries and packages in Python. Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cf106bf-8768-485b-94ec-bea3095b3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'reader', 'of', 'this', 'course', 'should', 'have', 'a', 'basic', 'knowledge', 'of', 'the', 'Python', 'programming', 'language', '.', 'He/she', 'must', 'have', 'knowledge']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_word_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7099296d-8ffe-41ba-970d-153374a25665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('reader', 'NN'), ('of', 'IN'), ('this', 'DT'), ('course', 'NN'), ('should', 'MD'), ('have', 'VB'), ('a', 'DT'), ('basic', 'JJ'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Python', 'NNP'), ('programming', 'NN'), ('language', 'NN'), ('.', '.'), ('He/she', 'NNP'), ('must', 'MD'), ('have', 'VB'), ('knowledge', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('types', 'NNS'), ('in', 'IN'), ('Python.He', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('write', 'VB'), ('functions', 'NNS'), (',', ','), ('and', 'CC'), ('also', 'RB'), ('have', 'VBP'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('import', 'NN'), ('and', 'CC'), ('use', 'NN'), ('libraries', 'NNS'), ('and', 'CC'), ('packages', 'NNS'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.'), ('Familiarity', 'NN'), ('with', 'IN'), ('basic', 'JJ'), ('linguistics', 'NNS'), ('and', 'CC'), ('probability', 'NN'), ('is', 'VBZ'), ('assumed', 'VBN'), ('although', 'IN'), ('not', 'RB'), ('required', 'VBN'), ('to', 'TO'), ('fully', 'RB'), ('complete', 'VB'), ('this', 'DT'), ('course', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# (6)  Apply PoS tags\n",
    "print(pos_tag(corrected_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20d4790a-191b-4938-be6b-6baab1f96ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(word_list):\n",
    "    corrected_word_list_without_stopwords = []\n",
    "    for wd in word_list:\n",
    "        if wd not in stop_words:\n",
    "            corrected_word_list_without_stopwords.append(wd)\n",
    "    return corrected_word_list_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9512f60-b240-4990-bfc0-6784ca0d8af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'reader',\n",
       " 'course',\n",
       " 'basic',\n",
       " 'knowledge',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language',\n",
       " '.',\n",
       " 'He/she',\n",
       " 'must',\n",
       " 'knowledge',\n",
       " 'data',\n",
       " 'types',\n",
       " 'Python.He',\n",
       " 'able',\n",
       " 'write',\n",
       " 'functions',\n",
       " ',',\n",
       " 'also']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word_list_without_stopwords = remove_stop_words(corrected_word_list)\n",
    "corrected_word_list_without_stopwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "821c84d3-fec0-4405-a248-b6ae891e2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Apply stemming and lemmatization\n",
    "stemmer = stem.PorterStemmer()\n",
    "def get_stems(word_list):\n",
    "    corrected_word_list_without_stopwords_stemmed = []\n",
    "    for wd in word_list:\n",
    "        corrected_word_list_without_stopwords_stemmed\\\n",
    "        .append(stemmer.stem(wd))\n",
    "    return corrected_word_list_without_stopwords_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3706ddf6-610c-4894-9fda-78793f8b79c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'reader',\n",
       " 'cours',\n",
       " 'basic',\n",
       " 'knowledg',\n",
       " 'python',\n",
       " 'program',\n",
       " 'languag',\n",
       " '.',\n",
       " 'he/sh',\n",
       " 'must',\n",
       " 'knowledg',\n",
       " 'data',\n",
       " 'type',\n",
       " 'python.h',\n",
       " 'abl',\n",
       " 'write',\n",
       " 'function',\n",
       " ',',\n",
       " 'also']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word_list_without_stopwords_stemmed = \\\n",
    "get_stems(corrected_word_list_without_stopwords)\n",
    "corrected_word_list_without_stopwords_stemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f894d9b5-1c3e-46e5-926d-64d2957d6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_lemma(word_list):\n",
    "    corrected_word_list_without_stopwords_lemmatized = []\n",
    "    for wd in word_list:\n",
    "        corrected_word_list_without_stopwords_lemmatized\\\n",
    "        .append(lemmatizer.lemmatize(wd))\n",
    "    return corrected_word_list_without_stopwords_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf9f6114-1a7d-48bb-aca0-62e7f9307b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'reader',\n",
       " 'cours',\n",
       " 'basic',\n",
       " 'knowledg',\n",
       " 'python',\n",
       " 'program',\n",
       " 'languag',\n",
       " '.',\n",
       " 'he/sh',\n",
       " 'must',\n",
       " 'knowledg',\n",
       " 'data',\n",
       " 'type',\n",
       " 'python.h',\n",
       " 'abl',\n",
       " 'write',\n",
       " 'function',\n",
       " ',',\n",
       " 'also']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_word_list_without_stopwords_lemmatized = \\\n",
    "get_lemma(corrected_word_list_without_stopwords_stemmed)\n",
    "corrected_word_list_without_stopwords_lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a873b46-17f5-4469-aa72-abfbe099477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The reader of this course should have a basic knowledge of the Python programming language.', 'He/she must have knowledge of data types in Python.He should be able to write functions, and also have the ability to import and use libraries and packages in Python.', 'Familiarity with basic linguistics and probability is assumed although not required to fully complete this course.']\n"
     ]
    }
   ],
   "source": [
    "# (8) Detect the sentence boundaries in the given text corpus \n",
    "print(sent_tokenize(corrected_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff085b5b-ed87-47a3-9154-d65dbb9fc3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b8ab2-b575-4af6-b8ec-fa2727121263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039187e-ced1-48e2-9f97-1937ba77223b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
